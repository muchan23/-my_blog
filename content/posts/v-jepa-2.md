---
title: "V-JEPA 2 を試してみた"
date: "2026-01-27"
tags: [世界モデル]
description: "V-JEPA 2 の機能紹介と実験結果をまとめます。"
---

## はじめに

（導入文をここに書きます）

## V-JEPA 2 で何ができるか

### 概要

V-JEPA 2 は、Meta が開発した自己教師あり学習ベースの動画エンコーダです。100万時間以上のインターネット動画を使い、Joint-Embedding Predictive Architecture（JEPA）により、物理世界の「理解」「予測」「計画」を可能にする世界モデルを構築しています。言語による教師信号なしで学習したにもかかわらず、LLM と連携することで動画質問応答タスクで SOTA を達成しています。

### 主な機能

1. **動画理解（Motion Understanding）**
   - 人間の動作やジェスチャーを高精度で認識
   - Something-Something v2 で **77.3%** の top-1 精度
   - Diving-48、Jester などの動作認識タスクでも高性能

2. **行動予測（Action Anticipation）**
   - 動画の文脈から1秒後の行動を予測
   - Epic-Kitchens-100 で **39.7 recall-at-5**（従来手法 PlausiVL 比 **+44%** 向上）
   - 300M パラメータでも 8B パラメータの既存手法を上回る

3. **動画質問応答（Video QA）**
   - LLM（Llama 3.1 8B）と連携し、動画に関する自然言語での質問に回答
   - 8B パラメータクラスで複数ベンチマークで SOTA:
     - PerceptionTest: **84.0**（物理・記憶・抽象化などのスキルを評価）
     - MVP: **44.5**（物理世界理解、バイアス軽減評価）
     - TempCompass: **76.9**（時間的理解）
     - TemporalBench: **36.7**
     - TOMATO: **40.3**

4. **ゼロショットロボット制御（V-JEPA 2-AC）**
   - 62時間のラベルなしロボット動画（Droid データセット）で追加学習
   - 新しい環境・新しい物体でもゼロショットで動作
   - 画像ゴールに基づく計画でタスクを実行:
     - Reach: **100%** 成功率
     - Grasp: **25-75%** 成功率
     - Pick-and-Place: **65-80%** 成功率

### ユースケース

- 動画内の人間行動の分類・認識（スポーツ分析、監視など）
- キッチンや工場での次の行動予測（1秒先を予測）
- 動画に関する自然言語での質問応答（動画検索、コンテンツ理解）
- 画像ゴールに基づくロボットアームの自律制御
- 物体の把持・移動・配置（Pick-and-Place）タスク
- タスク固有の報酬設計なしでのロボット学習

### 既存手法との違い

| 観点 | V-JEPA 2 | 既存手法 |
|------|----------|----------|
| 学習方法 | 自己教師あり（ラベル不要） | 教師あり or 報酬ベース |
| 言語監督 | 不要（後からLLMと連携可能） | 画像-テキストペアで事前学習（CLIP等） |
| 予測空間 | 表現空間（効率的・予測可能な特徴に集中） | ピクセル空間（計算コスト高・不要な詳細も予測） |
| ロボット学習データ | 62時間のラベルなし動画 | 大量のラベル付きデータ |
| 計画時間 | **16秒/アクション** | Cosmos: 4分/アクション（15倍遅い） |
| Pick-and-Place 成功率 | **65-80%** | Octo: 10-15% |
| Video QA (PerceptionTest) | **84.0** | PLM 8B: 82.7, Qwen2.5VL: 70.5 |

## 実験してみた

<!-- 実験結果をここに追記 -->

## 技術的な詳細

<!-- /paper-technical で生成した内容をここに追記 -->

## まとめ

（まとめをここに書きます）
