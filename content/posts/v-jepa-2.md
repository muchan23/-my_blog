---
title: "V-JEPA 2 を試してみた"
date: "2026-01-27"
tags: [世界モデル]
description: "V-JEPA 2 の機能紹介と実験結果をまとめます。"
---

## はじめに

（導入文をここに書きます）

## V-JEPA 2 で何ができるか

### 概要

V-JEPA 2 は、100万時間以上のインターネット動画で自己教師あり学習した動画エンコーダで、物理世界の理解・予測・計画を可能にする世界モデルです。少量のロボットデータ（62時間）で追加学習することで、ゼロショットでロボット制御にも応用できます。

### 主な機能

1. **動画理解（Motion Understanding）**: 人間の動作やジェスチャーを高精度で認識。Something-Something v2 で 77.3% の top-1 精度を達成
2. **行動予測（Action Anticipation）**: 人間の次の行動を予測。Epic-Kitchens-100 で 39.7 recall-at-5 を達成（従来手法比 44% 向上）
3. **動画質問応答（Video QA）**: 大規模言語モデル（LLM）と連携し、動画に関する質問に回答。PerceptionTest で 84.0、TempCompass で 76.9 の精度（8B パラメータクラスで SOTA）
4. **ゼロショットロボット制御**: V-JEPA 2-AC により、新しい環境でも物体の把持・移動・Pick-and-Place が可能。62時間のラベルなしロボット動画のみで学習

### ユースケース

- 動画内の人間行動の分類・認識
- 次に起こる行動の予測（キッチンでの作業など）
- 動画に関する自然言語での質問応答
- 画像ゴールに基づくロボットアームの自律制御
- 物体の把持・移動・配置（Pick-and-Place）タスク

### 既存手法との違い

| 観点 | V-JEPA 2 | 既存手法 |
|------|----------|----------|
| 学習方法 | 自己教師あり（ラベル不要） | 教師あり or 報酬ベース |
| 予測空間 | 表現空間（効率的） | ピクセル空間（計算コスト高） |
| ロボット学習データ | 62時間のラベルなし動画 | 大量のラベル付きデータ |
| 計画時間 | 16秒/アクション | Cosmos: 4分/アクション |
| Pick-and-Place 成功率 | 65-80% | Octo: 10-15% |

## 実験してみた

<!-- 実験結果をここに追記 -->

## 技術的な詳細

<!-- /paper-technical で生成した内容をここに追記 -->

## まとめ

（まとめをここに書きます）
