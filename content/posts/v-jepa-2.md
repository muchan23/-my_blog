---
title: "V-JEPA 2 を試してみた"
date: "2026-01-27"
tags: [世界モデル]
description: "V-JEPA 2 の機能紹介と実験結果をまとめます。"
---

## はじめに

（導入文をここに書きます）

## V-JEPA 2 で何ができるか

### 概要

V-JEPA 2 は、Meta が開発した自己教師あり学習ベースの動画エンコーダです。100万時間以上のインターネット動画を使い、Joint-Embedding Predictive Architecture（JEPA）により、物理世界の「理解」「予測」「計画」を可能にする世界モデルを構築しています。言語による教師信号なしで学習したにもかかわらず、LLM と連携することで動画質問応答タスクで SOTA を達成しています。

### 主な機能

1. **動画理解（Motion Understanding）**
   - 人間の動作やジェスチャーを高精度で認識
   - Something-Something v2 で **77.3%** の top-1 精度
   - Diving-48、Jester などの動作認識タスクでも高性能

2. **行動予測（Action Anticipation）**
   - 動画の文脈から1秒後の行動を予測
   - Epic-Kitchens-100 で **39.7 recall-at-5**（従来手法 PlausiVL 比 **+44%** 向上）
   - 300M パラメータでも 8B パラメータの既存手法を上回る

3. **動画質問応答（Video QA）**
   - LLM（Llama 3.1 8B）と連携し、動画に関する自然言語での質問に回答
   - 8B パラメータクラスで複数ベンチマークで SOTA:
     - PerceptionTest: **84.0**（物理・記憶・抽象化などのスキルを評価）
     - MVP: **44.5**（物理世界理解、バイアス軽減評価）
     - TempCompass: **76.9**（時間的理解）
     - TemporalBench: **36.7**
     - TOMATO: **40.3**

4. **ゼロショットロボット制御（V-JEPA 2-AC）**
   - 62時間のラベルなしロボット動画（Droid データセット）で追加学習
   - 新しい環境・新しい物体でもゼロショットで動作
   - 画像ゴールに基づく計画でタスクを実行:
     - Reach: **100%** 成功率
     - Grasp: **25-75%** 成功率
     - Pick-and-Place: **65-80%** 成功率

### ユースケース

- 動画内の人間行動の分類・認識（スポーツ分析、監視など）
- キッチンや工場での次の行動予測（1秒先を予測）
- 動画に関する自然言語での質問応答（動画検索、コンテンツ理解）
- 画像ゴールに基づくロボットアームの自律制御
- 物体の把持・移動・配置（Pick-and-Place）タスク
- タスク固有の報酬設計なしでのロボット学習

### 既存手法との違い

| 観点 | V-JEPA 2 | 既存手法 |
|------|----------|----------|
| 学習方法 | 自己教師あり（ラベル不要） | 教師あり or 報酬ベース |
| 言語監督 | 不要（後からLLMと連携可能） | 画像-テキストペアで事前学習（CLIP等） |
| 予測空間 | 表現空間（効率的・予測可能な特徴に集中） | ピクセル空間（計算コスト高・不要な詳細も予測） |
| ロボット学習データ | 62時間のラベルなし動画 | 大量のラベル付きデータ |
| 計画時間 | **16秒/アクション** | Cosmos: 4分/アクション（15倍遅い） |
| Pick-and-Place 成功率 | **65-80%** | Octo: 10-15% |
| Video QA (PerceptionTest) | **84.0** | PLM 8B: 82.7, Qwen2.5VL: 70.5 |

## 実験してみた

<!-- 実験結果をここに追記 -->

## 技術的な詳細

### アーキテクチャ

V-JEPA 2 は **Vision Transformer（ViT）** をベースとしたエンコーダ・プレディクタ構造を採用しています。動画をパッチ（tubelet）に分割し、マスクされた部分の表現を予測することで学習します。

#### 主要コンポーネント

- **エンコーダ $E_\theta(\cdot)$**: 動画から特徴表現を抽出する ViT
- **プレディクタ $P_\phi(\cdot)$**: マスクされたパッチの表現を予測する ViT
- **3D-RoPE**: 時間・高さ・幅の3軸に対応した Rotary Position Embedding（従来の絶対位置埋め込みより大規模モデルで安定）
- **Tubelet**: 動画を $2 \times 16 \times 16$（時間×高さ×幅）のパッチに分割

#### モデルサイズ

| モデル | パラメータ数 | 幅 | 深さ | ヘッド数 | MLP次元 |
|--------|-------------|-----|------|---------|---------|
| ViT-L | 300M | 1024 | 24 | 16 | 4096 |
| ViT-H | 600M | 1280 | 32 | 16 | 5120 |
| ViT-g | 1B | 1408 | 40 | 22 | 6144 |
| Predictor (ViT-s) | 22M | 384 | 12 | 12 | 1536 |

### 学習方法

#### 事前学習（Pretraining）

**Mask-Denoising in Representation Space** という手法で学習します。

- **目的関数**: マスクされたパッチの表現を L1 損失で予測
  $$\min_{\theta, \phi} \| P_\phi(\Delta_y, E_\theta(x)) - \text{sg}(E_{\bar{\theta}}(y)) \|_1$$
- **EMA Teacher**: エンコーダの重みの指数移動平均（EMA）をターゲットとして使用（表現崩壊を防止）
- **最適化手法**: AdamW
- **学習スケジュール**: Warmup → Constant → Cooldown（progressive resolution training）
  - Warmup: 12,000 steps（16フレーム、256×256）
  - Constant: 228,000 steps
  - Cooldown: 12,000 steps（64フレーム、384×384 に解像度を上げる）

#### V-JEPA 2-AC 追加学習（Post-training）

V-JEPA 2 のエンコーダを凍結し、アクション条件付きプレディクタを学習します。

- **目的関数**: Teacher-forcing loss + Rollout loss
  $$L(\phi) = \mathcal{L}_{\text{teacher-forcing}}(\phi) + \mathcal{L}_{\text{rollout}}(\phi)$$
- **アーキテクチャ**: 300M パラメータの Transformer（24層、16ヘッド、1024次元）
- **Block-causal attention**: 各タイムステップのパッチは、現在および過去のアクション・状態・パッチにのみアテンション可能
- **計画手法**: Cross-Entropy Method（CEM）でゴール画像に近づくアクション系列を最適化

### データセット

| 用途 | データセット名 | サンプル数 | 総時間 | 重み |
|------|---------------|-----------|--------|------|
| 事前学習 | SSv2 | 168K | 168時間 | 0.056 |
| 事前学習 | Kinetics | 733K | 614時間 | 0.188 |
| 事前学習 | HowTo100M | 1.1M | 134K時間 | 0.318 |
| 事前学習 | YT-Temporal-1B（キュレーション済） | 19M | 1.6M時間 | 0.188 |
| 事前学習 | ImageNet | 1M | - | 0.250 |
| ロボット学習 | Droid（raw） | - | 62時間 | - |

**データキュレーション**: YT-1B はノイズが多いため、DINOv2 で特徴抽出 → クラスタリング → Kinetics/SSv2 等に近いクラスタのみ選択

### ハイパーパラメータ

#### 事前学習

| パラメータ | Primary Phase | Cooldown Phase |
|-----------|---------------|----------------|
| フレーム数 | 16 | 64 |
| FPS | 4.0 | 4.0 |
| 解像度 | 256×256 | 256/384/512 |
| バッチサイズ | 3072 | 3072 |
| 学習率 | 1e-4 → 5.25e-4 | 5.25e-4 → 1e-6 |
| Weight Decay | 0.04 | 0.04 |
| EMA | 0.99925 | 0.99925 |
| Spatial Mask Scale | [0.15, 0.7] | [0.15, 0.7] |

#### V-JEPA 2-AC 追加学習

| パラメータ | 値 |
|-----------|-----|
| オプティマイザ | AdamW |
| バッチサイズ | 256 |
| 学習率 | 7.5e-5 → 4.25e-4 → 0 |
| Warmup | 4,500 steps |
| Constant | 85,500 steps |
| Cooldown | 4,500 steps |
| Weight Decay | 0.04 |

### 重要な設計上の選択

1. **表現空間での予測**: ピクセル空間ではなく表現空間で予測することで、予測可能な特徴（物体の軌道など）に集中し、予測不可能な詳細（草の葉の位置など）を無視できる

2. **Progressive Resolution Training**: 低解像度・短いクリップで大部分を学習し、最後の Cooldown フェーズでのみ高解像度・長いクリップを使用（8.4倍の計算効率化）

3. **3D-RoPE**: 絶対位置埋め込みの代わりに Rotary Position Embedding を使用することで、大規模モデルの学習が安定

4. **データキュレーション**: 無秩序な大規模データ（YT-1B）を、高品質なデータセット（Kinetics等）に類似したサンプルのみに絞り込み

5. **エンコーダ凍結**: ロボット学習時にエンコーダを凍結することで、少量のデータ（62時間）でも過学習せずに世界モデルを学習可能

## まとめ

（まとめをここに書きます）
